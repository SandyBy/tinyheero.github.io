---
title: "Basic Probability"
date: "March 17th, 2016"
layout: post
output:
  html_document:
tags: [R, stats]
---

Probabilites represent the chances of an event occurring. Given a random process (e.g. flipping a coin), probabilities can be numerically quantified by measuring the number of desired outcomes divided by the total number of outcomes. For instance, if we flip a coin the two possible outcomes are head or tails. The probability of getting a head is then 1/2.

Probabilites may be either joint, marginal or conditional.

```{r echo = FALSE}
library("knitr")
library("captioner")

tbl.nums <- captioner(prefix = "<u>Table</u>", css_class = "tblcaption")
```

## Joint Probabilities

The first type of probability we will discuss is the joint probability which measures the probability of two different events occurring at the same time. Let's use the diamonds dataset, from ggplot2, to measure the co-occurrence of diamond color and cut. We can represent these data using a "two-way table":

```{r, message = FALSE}
library("ggplot2")
library("dplyr")
library("reshape2")
library("knitr")

diamonds.color.cut.df <-
  diamonds %>%
  group_by(color, cut) %>%
  summarize(n = n())

diamonds.color.cut.df %>%
  dcast(color ~ cut, value.nar = "n") %>%
  kable(align = "l", format = "html",
        table.attr='class="table table-striped table-hover"')
```

Joint probabilities can be calculated by taking the proportion of times a specific color-cut combination occurs divided by total number of all color-cut combinations:

```{r}
diamonds.color.cut.prop.df <- 
  diamonds.color.cut.df %>%
  ungroup() %>%
  mutate(prop = n / sum(n))

diamonds.color.cut.prop.df %>%
  dcast(color ~ cut, value.var = "prop") %>%
  kable(align = "l", format = "html", 
        table.attr = 'class="table table-striped table-hover"')
```

`r tbl.nums("two_way.tbl.cap.lbl","Color-Cut Two Way Probability Table.")`

We can see that certain combinations are more probable than others. 

<div class="alert alert-dismissible alert-info">
<h4>Heads Up!</h4>
For brevity and simpler mathematical notions, for the rest of this post we will use the random variable X to represent color and Y to represent cut.
</div>

For instance, a diamond with the X = G and Y = cut , <span class="inlinecode">$P(X = G, Y= ideal) = 0.09$</span>, is much more probable than a diamond with X = D and Y = fair, <span class="inlinecode">$P(X = D, Y = fair) = 0.003$</span>.

## Marginal Probabilities

The second type of probability is the marginal probability. The interesting thing about a marginal probability is that the term sounds complicated, but it's actually the probability that we are most familiar with. Basically anytime you are in interested in a single event irrespective of any other event (i.e. "marginalizing the other event"), then it is a marginal probability. For instance, the probability of a coin flip giving a head is considered a marginal probability. Typically, we just say probability and not the marginal part of it because this part only comes into play when we have to factor in a second event. 

To illustrate this, we will go back to our diamond color-cut combination two-way table. We will use the random variable X to represent color and Y to represent cut. If we are interested in say the marginal probabilty P(X = "D"), then basically we are asking "what is the probability of getting a diamond that is color D irrespective its cut?" It should be initutive that we can calculate this information by simply summing up the joint probabilities of the row color D. Mathematically:

<div>
$$P(X = D) = \sum_{y \in S_{Y}}P(X = D, Y = y)$$
<div>

Where <span class="inlinecode">$S_{Y}$</span> represents all the possible values of the random variable Y. We can calculate the marginal probability of all the different colors. We can also calculate the marginal probability of cut by using the same logic and summing up the joint probabilities of the columns. For instance, to calculate <span class="inlinecode">P(Y = Fair)</span>,

<div>
$$P(Y = Fair) = \sum_{x \in S_{X}}P(X = x, Y = Fair)$$
<div>

Let's add the marginal probabilities to the two way table now:

```{r}
color.marginal.df <- 
  diamonds.color.cut.prop.df %>%
  group_by(color) %>%
  summarize(marginal = sum(prop))

cut.marginal.df <- 
  diamonds.color.cut.prop.df %>%
  group_by(cut) %>%
  summarize(marginal = sum(prop))

diamonds.color.cut.prop.df %>%
  dcast(color ~ cut, value.var = "prop") %>%
  left_join(color.marginal.df, by = "color") %>%
  bind_rows(
    cut.marginal.df %>%
    mutate(color = "marginal") %>%
    dcast(color ~ cut, value.var = "marginal")
  ) %>%
  kable(align = "l", format = "html",
        table.attr = 'class="table table-striped table-hover"')
```

`r tbl.nums("two_way_marginal.tbl.cap.lbl", "Color-Cut Two Way Probability Table with Marginal Probabilites.")`

## Conditional Probability

The final type of probability is the conditional probability. A conditional probability is the probability of an event X occurring when a secondary event Y is true. Mathematically, it is represented as <span class="inlinecode">$P(X | Y)$</span>. This is read as "probability of X given/conditioned on Y".

For example, if someone asked you the probability of getting a diamond with the G color - P(X = G)? you could use the `r tbl.nums("two_way_marginal.tbl.cap.lbl", display = "cite")` to find the marginal probability of this event. But what if you had an additional layer of information where you knew that the diamond was also of ideal cut? This slightly changes things as you have a bit more information to work it. 

To work this out, we essential need pieces of information:

1. <span class="inlinecode">$P(Y = ideal)$</span>: Marginal probability of Y = ideal.
1. <span class="inlinecode">$P(X = G, Y = ideal)$</span>: Joint probability of X = G and Y = ideal.

P(X = G) acts as our baseline (denominator) here so we can calculate the conditional probability as follows:

<div>
$$P(X = G | Y = ideal) = \frac{P(X = G, Y = ideal)}{P(Y = ideal)}$$
<div>

So the conditional probability would be in this case:

```{r}
joint.prob <- 
  diamonds.color.cut.prop.df %>%
  filter(color == "G", cut == "Ideal") %>%
  .$prop

marg.prob <- 
  cut.marginal.df %>%
  filter(cut == "Ideal") %>%
  .$marginal

cond.prob <- joint.prob / marg.prob
cond.prob
```

So basically if we didn't factor it any other information, our P(X = G) was
`r color.marginal.df %>% 
     filter(color == "G") %>%
     .$marginal
`.
But once we factored in an additional level of information which was Y = ideal, our probability changed to `r cond.prob` as we had more information.

Put another way, we had a "reallocation of our belief" in an event once we factored in additional information. This is actually where we start to venture into a bit of "bayesian" statistics thinking. But we will save that for a subsequent post.

## What about Continuous Random Variables?

In this post's example dataset of diamonds, we used the random variables X and Y to represent diamond colors and cuts respectively. Both of which are discrete random variables. If dealing with continuous random variables, these probabilites still exist with the exception that we are dealing with integrals instead of summation. This means for the mathematical represents of marginal probabilities:

<div>
$$P(X = D) = \int_{}P(X = D, Y = y)$$
<div>

## 

## 

## References

* [Nicholas School Statistics Review - Joint, Marginal and Conditional Probabilities](http://sites.nicholas.duke.edu/statsreview/probability/jmc/)
* [Chapter 4 - Doing Bayesian Data Analysis - A Tutorial with R, JAGS, and Stan](https://sites.google.com/site/doingbayesiandataanalysis/)

