---
title: "What is a Probability Distribution?"
layout: post
output: 
  html_document:
tags: []
---

```{r echo = FALSE}
library("knitr")
library("captioner")
library("magrittr")
library("captioner")
#knitr::opts_chunk$set(fig.path="{{ site.url }}/assets/gmm-em/")

fig_nums <- captioner(prefix = "<u>Figure</u>", css_class = "figcaption")
```

A probability distribution is a way to represent the possible values and the respective probabilities of a random variable. There are two types of probability distributions: discrete and continuous probability distribution. As you might guess, a discrete probability distribution is used when we have a discrete random variable. A continuous probability distribution is used when we have a continuous random variable. 

In this post, we will explore what discrete and continuous probability distributions are. Additionally, we will describe that a probability mass and density function, their key properties and how they relate to probability distributions. Here is an overview of what will be discussed in this post.

**Table of Contents**

<ul data-toc="body" data-toc-headings="h2,h3"></ul>

## Discrete Probability Distribution

In a previous post on [random variables]({% post_url 2016-02-26-random-variables %}), I had used the example of a random process of flipping a coin x number of times and measuring the total of heads using a discrete random variable Y. As an example, let us try to build a probability distribution from a random process like this where we are flipping a coin 3 times. This random process can have a total of 8 possible outcomes:

<div class="alert alert-dismissible alert-warning">
<h4>Heads Up!</h4>
[I suggest watching this video](https://www.youtube.com/watch?v=5lpqiGixDd0) if you are unclear on how these outcomes were generated.
</div>

1. HHH
1. HHT
1. HTH
1. HTT
1. THH
1. THT
1. TTH
1. TTT

We let our random variable Y serve as a way to map the number of heads we get to a numeric value. So the most initutive way would be:

1. Y = 0 if we get no heads.
1. Y = 1 if we get 1 head.
1. Y = 2 if we get 2 heads.
1. Y = 3 if we get 3 heads.

So now that we have this random variable and the possible values this variable can take, let's try to figure out the associated probabilities of each outcome. 

1. <span class="inlinecode">$P(Y = 0)$</span>: The only way we can get 0 heads is if all 3 coin flips gives a tail. The only outcome that satisfies this is the TTT outcome. This means the probability <span class="inlinecode">$P(Y = 0) = \frac{1}{8}$</span>.
1. <span class="inlinecode">$P(Y = 1)$</span>: To get 1 head, the outcomes HTT, THT, and TTH satisfy this. So this means <span class="inlinecode">$P(Y = 1) = \frac{3}{8}$</span>.
1. <span class="inlinecode">$P(Y = 2)$</span>: To get 2 heads, the outcomes HHT, HTH, and THH satisfy this. So this means <span class="inlinecode">$P(Y = 2) = \frac{3}{8}$</span>.
1. <span class="inlinecode">$P(Y = 3)$</span>: The only way we can get 3 heads is if all 3 coin flips gives a head The only outcome that satisfies this is the TTT outcome. This means the probability <span class="inlinecode">$P(Y = 3) = \frac{1}{8}$</span>.

What we have just described is called a "**probability mass function (pmf)**" which is a function that defines the probability of a discrete random variables taking on a given value. When we take all the possible values and associated probabilities into consideration, this is called a discrete probability distribution (as defined by a pmf). We can visualize this particular pmf as follows:

```{r prob_distr_example, warning = FALSE, message = FALSE}
library("ggplot2")
library("dplyr")

prob.distr.df <- data_frame(value = c(0, 1, 2, 3),
                            prob = c(1/8, 3/8, 3/8, 1/8))

prob.distr.df %>%
  ggplot(aes(x = value, y = prob)) +
  geom_bar(stat = "identity") +
  ylim(c(0, 1)) +
  xlab("Y (Number of Heads)") + 
  ylab("Probability")
```

`r fig_nums("prob_distr_example.fig.cap", "Discrete Probability Distribution of Random Variable Y. The x-axes shows the different outcomes of the random variable while the y-axes shows the corresponding probabilities of these outcomes.")`

You might sometimes see the term probability distribution table. This is just the same thing as a pmf. The name stems from the fact that there are a finite number of outcomes and and so we can represent these outcomes and their associated probabilities in a finite table. As we will see below, we can't do this for a continuous random variable hence why a probabilty distribution table only has meaning in the context of a discrete random variable.

## Continuous Probability Distribution

In the example above, Y was a discrete random variable. When the outcomes are discrete we have the ability to directly measure the probability of each outcome. Any probability distributions based on a discrete random variable is classified as a "discrete probability distribution" (which is what `r fig_nums("prob_distr_example.fig.cap", display = "cite")` is).

When the random variable is continuous, then things get a little more complicated. **We are not able to directly measure the probability of a specific continuous value**. This may seem a big confusing at first, but imagine you had a random variable X that measured the price of a diamond. Now what if someone asked you the following question: If you sampled a single diamond, what is the probability that its **exact** price is \$326.57? Not \$326.58 or \$326.56, but exactly \$326.57? 

If you think of it that way, then the probability of getting a diamond with that exact price is probably really low. In fact, the probability of any exact price is really low. **This is why the concept of probability in the continous scale doesn't make sense**. Instead, what we do is "discretize" the sample space so that we can work in intervals instead of individual outcome values. To make this more concrete, we will use the diamond dataset from ggplot2 to illustrate this example. First, let's plot a summary histogram of the diamond prices for 53940 diamonds and use interval sizes of 100 (to represent \$100 intervals).

```{r, warning = FALSE, message = FALSE}
diamonds %>%
  ggplot(aes(x = price)) +
  geom_histogram(binwidth = 100) +
  xlab("X (Diamond Price)") +
  ylab("Number of Diamonds")
```

Once we have these bins of data, we can start talking about proportion of samples falling into bins. For instance, we can ask the question what is the probability of a diamond having a price between 1000 and 1100:

```{r}
num.diamonds.in.bin <- 
  diamonds %>%
  filter(price > 1000, price < 1100) %>%
  nrow()
```

A total of `r num.diamonds.in.bin` diamonds fall in this bin which equates to a probability of:

```{r}
prob.mass <- num.diamonds.in.bin / nrow(diamonds)
prob.mass
```

When we talk about the probability of a discrete outcome falling into a bin like this, then this type of probability is called a "probability mass". As the probability mass is dependent on the bin width, the "probability density" is used to represent the ratio of the probability mass to bin width (interval):

```{r}
prob.dens <- prob.mass / 100
prob.dens
```

To get more precision, we would want our intervals to be small since wide intervals are not very informative. Ideally, our intervals should be infinitesimally small. When we do this, we produce something that starts to resemble a "curve" (here we use [kernel density estimation](https://en.wikipedia.org/wiki/Kernel_density_estimation) to estimate the curve).

```{r}
diamonds %>%
  ggplot(aes(x = price)) +
  geom_density() +
  xlab("X (Diamond Price)") +
  ylab("f(X)")
```

This "curve" is what is called a probability density function (pdf) which is used to describe the probability distribution of a continuous random variable. 

### Properties of Probability Mass/Density Functions

There are a few key properites of a pmf, <span class="inlinecode">$f(X)$</span>:

1. <span class="inlinecode">$f(X = x) > 0$ where <span class="inlinecode">$x \in S_{X}$</span> (<span class="inlinecode">$S_{X}$</span> = sample space of X).

1. Since we can directly measure the probability of an event for discrete random variables, then 

    <div>
    $$P(X = x) = f(X = x)$$
    </div>

1. The probability of all possible events must sum to 1:

    <div>
    $$\sum_{x \in S_{X}} f(X) = 1$$
    </div>

The key properites of a pdf, <span class="inlinecode">f(Y)</span>, are very similar to a pmf. The big difference is that **we need to think in terms of intervals instead of individual outcomes**. This means we have to work with integrals and not summations:

1. <span class="inlinecode">$f(Y = y) > 0$ where <span class="inlinecode">$y \in S_{Y}$</span> (<span class="inlinecode">$S_{Y}$</span> = sample space of Y). This property is the same as for a pmf.

1. The probability of a value y belonging to an interval [a, b] is:

    <div>
    $$P(a \leq y \leq b) = \int_{a}^{b} f(Y) \,dy$$
    </div>

    In other words, the probability of <span class="inlinecode">$x \in [a, b]$</span> is equivalent to taking the integral of the pdf between a and b.

1. The entire area under the pdf must sum to 1.

    <div>
    $$\int_{-\infty}^{\infty} f(Y) \,dy = 1$$
    </div>

## Summary

The big take home messages are as follows:

* A probability distribution is a way to represent the possible values and the respective probabilities of a random variable. There are two types of probability distributions:
    + Discrete probability distribution for discrete random variables.
    + Continuous probability distribution for continuous random variables.
* We can directly calculate probabilites of a discrete random variable, X = x, as the proportion of times the x value occurs in the random process.
* Probabilites of a continuous random variable, Y = y, are **not** directly measureable. Instead, we calculate the probability as the proportion of times y occurs in an interval [a, b].
* Probability mass functions (pmf) is used to describe a discrete probability distribution. While a probability density functions (pdf) is used to describe a continuous probability distribution.
  
## References

* [Doing Bayesian Data Analysis - A Tutorial with R, JAGS, and Stan](https://sites.google.com/site/doingbayesiandataanalysis/)
* [Probability Distribution Table - Intro with tossing a coin 3 times](https://www.youtube.com/watch?v=5lpqiGixDd0)
* [What is a Probability Distribution?](http://stattrek.com/probability-distributions/probability-distribution.aspx)
* [Continuous Probability Distribution](http://stattrek.com/statistics/dictionary.aspx?definition=Continuous%20probability%20distribution)
* [Khan Academy - Probability density function](https://www.youtube.com/watch?v=Fvi9A_tEmXQ)
* [PennState STAT 414/415 - Probability Density Functions](https://onlinecourses.science.psu.edu/stat414/node/97)
* [What is the relationship between the probability mass, density, and cumulative distribution functions?](https://www.quora.com/What-is-the-relationship-between-the-probability-mass-density-and-cumulative-distribution-functions)

## R Session

```{r session}
devtools::session_info()
```
