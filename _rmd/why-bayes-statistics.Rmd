---
title:  "Why Bayesian Statistics?"
date: "December 14th, 2018"
layout: post
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    code_folding: hide
tags: [R, stats]
---

```{r echo = FALSE}
# Keep this when you are rendering this rmarkdown not through the r-to-jekyll.R
# script
library("knitr")
opts_chunk$set(dev = c("CairoSVG"))
```

The term you will have undoubtly encountered the term Bayesian statistics at some point. It is What makes it so 
Having done my 

The crux of Bayesian statistics is centered around representing uncertainity about an unknown quantity. To illustrate this, imagine I had the following coin (this is a Canadian penny):

<div style="text-align:center">
  <img src="https://upload.wikimedia.org/wikipedia/en/f/f8/Canadian_Penny_-_Obverse.png"/>
</div>

If I flipped this coin, what is the probability that the coin lands on a head? You most likely guessed 0.5, which is a reasonable guess given your prior knowledge on how coins work. But what if I told you that I got this coin from a magic shop? You'll probably have some doubts that it is 0.5 now. It could be anything now. Maybe it's a trick coin and always gives you head (i.e. 1 probability) ? Or maybe it always gives you a tail (i.e. 0 probability)? Or maybe it is biased towards ahead (e.g. 0.75 probability). The point is there is some uncertainity in your estimation of this unknown quantity. 

Let's say I flipped this coin n times and it came back with r heads, and then I asked you this question:

<div>
$$P(\theta_{1} < \theta < \theta_{2} | n, r_{n})$$
</div>

Verbosely put, what's the probability that this coin gives a head (<span class="inlinecode">$\theta$</span>) is between <span class="inlinecode">$\theta_{1}$</span> and <span class="inlinecode">$\theta_{2}$</span> given you have <span class="inlinecode">$n$</span> flips and <span class="inlinecode">$r_{n}$</span> heads. In classical/frequentist statistics, this question actually makes no sense. This is because in classical statistics, parameters (unknown quantities) are fixed and have no uncertainity in their value; They are either that value or they are not. But in a Bayesian world, we are never completely certain about any estimations. As such, all estimations of an unknown quantity (e.g. the probability that a coin gives a head) has uncertain.

# How do we represent these uncertainities?

Uncertainities are expressed as a [probability distributions]({% post_url 2016-03-17-prob-distr %}). 
For instance, imagine you had the following probability distribution:

```{r uncertainty_as_prob_distr, fig.height = 5}
library("magrittr")
library("ggplot2")

data.frame(x = c(0, 1)) %>%
  ggplot(aes(x)) +
  stat_function(fun = dbeta, n = 101, args = list(shape1 = 5, shape2 = 5)) +
  ylim(c(0, 2.8)) +
  xlab("Probability of a head") +
  ylab("Density") +
  ggtitle("Uncertainity of a coin's probability of giving a head") +
  # Credible interval arrow
  geom_segment(
    aes(x = 0.26, xend = 0.74, y = 0.75, yend = 0.75),
    arrow = arrow(length = unit(0.2, "cm"))
  ) +
  geom_segment(
    aes(x = 0.74, xend = 0.26, y = 0.75, yend = 0.75),
    arrow = arrow(length = unit(0.2, "cm"))
  ) +
  geom_label(aes(x = 0.5, y = 0.9, label =  "90% credible interval")) +
  # 95% area < 0.75
  geom_segment(
    aes(x = 0.74, xend = 0.125, y = 0.05, yend = 0.05),
    arrow = arrow(length = unit(0.2, "cm"))
  ) +
  geom_label(aes(x = 0.5, y = 0.2, label =  "95% area < 0.75")) +
  # MAP value
  geom_segment(
    aes(x = 0.6, xend = 0.5, y = 2.6, yend = 2.5),
    arrow = arrow(length = unit(0.2, "cm"))
  ) +
  geom_label(
    aes(
      x = 0.8, y = 2.6,
      label = "Most likely value (maximum a\n posterior probability (MAP)\n estimate)"
    )
  )
```

The x-axis represents the plausible values that the probability of a head could 
take. The y-axis represents the "confidence" (this isn't entirely accurate in 
mathematical terms, but will suffice for this example) that the probability of a 
head takes this value. By expressing our uncertainity as a probability 
distribution, we get these benefits:

* The x value with the highest density peak represents the most likely value. In
  this case, that would be 0.5.
* Credible intervals (CI) can be formed. For instance, [0.25 - 0.75] forms a 90% 
  CI that tells us we are 90% confident that the parameter is in this interval. 
  This is quite different from a confidence interval from classical statistics, 
  which is actually quite a counter-inituitive statistic (see my "[How do I Interpret a Confidence Interval? post"]({% post_url 2015-08-25-how-to-interpret-a-CI %}))
* No "p-value" calculation is required. Just calculate the relevant tail areas. 
  For instance, what is P(head) < 0.75? We just look at the area left of 0.75,
  which ends up being 0.95 of the total area. So we can say there is a 95% 
  chance of P(head) < 0.75. 
* There is a technique called Bayesian inference that allows us to adapt the 
  distribution in light of additional evidence. (see my "[How to Do Bayesian Inference 101"]({% post_url 2017-03-08-how-to-bayesian-infer-101 %}) 
  post for more details on this)

# Where do these probability distributions come from?

While you could theoreticaly make your own probability distributions, in 
practice people use established probability distributions (e.g. beta, normal).
For instance, here are 6 different beta distributions:

```{r beta, fig.asp = 0.8, fig.width = 8, message = FALSE, fig.cap = "Different beta distributions"}
library("tibble")
library("cowplot")
library("glue")

# Cowplot overrides the default ggplot2 theme. This sets it back to the default
# theme.
theme_set(theme_grey())

beta_params_df <-
  tribble(
    ~ beta_distr_num, ~ shape1, ~ shape2,
                   1,      0.5,      0.5, 
                   2,        1,        1, 
                   3,        5,        1, 
                   4,        5,        5, 
                   5,        5,       20, 
                   6,        50,     200
  )

#' Plots a beta distribution 
#'
#' @param in_data List of arguments for the beta density function (dbeta)
#' @return ggplot plot
plot_beta <- function(in_data) {
  # Create aliases for easier reference below
  cur_shape_1 <- in_data[["shape1"]]
  cur_shape_2 <- in_data[["shape2"]]

  data.frame(x = c(0, 1)) %>%
    ggplot(aes(x)) +
    stat_function(
      fun = dbeta, 
      n = 101, 
      args = list(shape1 = cur_shape_1, shape2 = cur_shape_2)
    ) +
    ylab("Density") +
    xlab("Probability of a head") +
    ggtitle(glue("Beta({cur_shape_1}, {cur_shape_2})"))
}

beta_plots <- 
  split(beta_params_df, seq_len(nrow(beta_params_df))) %>%
  lapply(plot_beta)

plot_grid(plotlist = beta_plots, ncol = 3, nrow = 2, align = "v")
```

**Each of these beta distributions represents a different prior knowledge**. 
For each, Beta(0.5, 0.5) represents a belief that the coin always gives heads or
never gives head. While Beta(1, 1) represents a global uncertainity in that the 
probability of a head could be any value (this is often referred to as an
uniform prior).
coin 


# Conclusions

