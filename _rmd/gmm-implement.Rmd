---
title: "Implementating Your Own Mixture Model in R"
author: "Fong Chun Chan"
date: "August 25, 2015"
output: 
  html_document:
    toc: true
tags: [R, mixmodels, expectation_maximization]
---

In my previous post ["Using Mixture Models for Clustering in R"]({% post_url 2015-10-13-mixture-model %}), I covered the concept of mixture models and how one could use a gaussian mixture model (GMM), one type of mixure model, for clustering. If you are like me, not knowing what is happening "under the hood" may bug you. What is actually happening when I run the `normalmixEM` function from mixtools?  How does it know where to "put" the components? 

The way to think about this problem is to understand that this is a parameter estimation (i.e. model selection) problem. We will first start this post by discussing a bit about parameter estimation and specifically in the context of two scenarios: Complete and Incomplete Data.

## Parameter Estimation in the "Complete Data" Scenario

Let's start with the scenario where you are given the following 1-dimensional data with the colors representing the "source" they are from:

```{r parameter_est_complete_data, dev = "png", message = FALSE}
library("ggplot2")
library("dplyr")
library("reshape2")
library("mixtools")

options(scipen = 999)

comp1.vals <- data_frame(comp = "A", 
                         vals = rnorm(50, mean = 1, sd = 0.5))
comp2.vals <- data_frame(comp = "B", 
                         vals = rnorm(50, mean = 1.5, sd = 0.5))

vals.df <- bind_rows(comp1.vals, comp2.vals)

vals.df %>%
  ggplot(aes(x = vals, y = 0, color = factor(comp))) +
  geom_point(alpha = 0.4) +
  scale_color_discrete(name = "Source of Data")
```

Let's say you believe the data source is actually a gaussian distribution. Then with these values and knowledge of their data source, how could you estimate the parameters of the gaussian distribution $\mathcal{N}(X|\mu,\sigma$)? In other words, what parameters most likely gave rise to the data we are observing. 

We can use [maximum likelihood estimation](https://en.wikipedia.org/wiki/Normal_distribution#Estimation_of_parameters) to estimate the mean and variance:

* $\mu_{k} = \frac{\sum_{i}^{N_{k}}x_{i,k}}{N_{k}}$
* $\sigma_{k} = \frac{\sum_{i}^{N_{k}}(x_{i,k} - \mu_{k})^2}{N_{k}}$

Let's give this a try and see what values we get:

```{r}
vals.df %>%
  group_by(comp) %>%
  summarize(mean_vals = mean(vals),
            sd_vals = sd(vals))
```

This is very close to the true parameters of the gaussian which were mean, standard deviation of (1, 0.5), (1.5, 0.5) for the respective gaussians (NOTE: the more data we sample, the closer we get to the true parameters).

## Parameter Estimation in the "Incomplete Data" Scenario

Now let us consider the following scenario where you have the same data but you don't know the source now.

```{r}
vals.df %>%
  ggplot(aes(x = vals, y = 0)) +
  geom_point(alpha = 0.4)
```

Without the labels on the data (often called latent variables) indicating their source, we are hopeless in applying MLE. But if we somehow find a way to "complete" the data (i.e. find the labels for each data point), then we can go back to using MLE. One way around this could be to:

1. Set some initial parameter estimates on your gaussians.
1. Assign (label) the data to one of the gaussians based on which one most likely generated the data.
1. Treat the labels as being correct and then use MLE to re-estimate the parameters for the different gaussians.
1. Repeat steps 2 and 3 until there is convergence.

This iterative approach is the basis for something called the expectation maximization (EM) algorithm. The main difference is rather than assigning the data to one of the possible gaussians, EM considers the possibilities of the data belonging to all of the possible gaussians. This is analogous to the concept of "hard" and "soft" labeling that I mentioned in my previous post ["Using Mixture Models for Clustering in R"]({% post_url 2015-10-13-mixture-model %}). Working in a probablistic framework allows us to assign probabilities (responsibilities) of data belonging to a gaussian so that we don't have to "commit" the data to any one gaussian. 

The best way to remember what EM is used for is as follows:

> The expectation maximization algorithm is a natural generalization of maximum likelihood estimation to the incomplete data case. -- Chuong B Do & Serafim Batzoglou. What is the expectation maximization algorithm? Nature Biotechnology. 2008.

In this post, we will use the EM algorithm to fit our GMM. 

## Fitting a GMM using Expectation Maximization 

The EM algorithm consists of 3 major steps:

1. Initialization
1. Expectation (E-step)
1. Maximization (M-step)

Steps 2 and 3 are repeated until convergence. We will cover each of these steps and how convergence is reached below. But first we must understand how to mathematically represent a GMM:

$$ P(X|\mu,\sigma,\alpha) = \sum_{k=1}^{K}\alpha_k\mathcal{N}(X|\mu_{k},\sigma_{k}^{2}) $$

* X = Dataset of n elements (x~1~, ..., x~n~).
* $\alpha_{k}$ = Mixing weight of the kth component. $\sum_{k=1}^{K}\alpha_{k} = 1$.
* $\mathcal{N}(x|\mu_{k},\sigma_{k})$ Gaussian probability density function (pdf) of the kth component defined by the parameters $\mu_{k}$ and $\sigma_{k}$.
* $\mu_{k}$ = Mean of the kth component.
* $\sigma_{k}^{2}$ = Variance of the kth component.

```{r, warning = FALSE, message = FALSE, echo = FALSE}
#' Plot a Mixture Component
#' 
#' @param x Input Data
#' @param mu Mean of Component
#' @param sigma Standard of Component
#' @param lam Mixture Weight of Component
plot_mix_comps <- function(x, mu, sigma, lam) {
  lam * dnorm(x, mu, sigma)
}

set.seed(1)
wait <- faithful$waiting
mixmdl <- normalmixEM(wait, k = 2)

post.df <- as.data.frame(cbind(x = mixmdl$x, mixmdl$posterior))
```

It looks complicated. but if we use the example from the previous post then this breaksdown to:

$$P(X) = `r round(mixmdl$lambda[1], 3)`\mathcal{N}(X|`r round(mixmdl$mu[1], 3)`, `r round(mixmdl$sigma[1], 3)`) + `r round(mixmdl$lambda[2], 3)`\mathcal{N}(X|`r round(mixmdl$mu[2], 3)`, `r round(mixmdl$sigma[2], 3)`)$$

All we've done here is substitute in the parameters of the mixture model. 

In case you were wondering what the $P(X|\mu,\sigma,\alpha)$ means, don't worry about that for now. We will explain exactly what this means later on in the post.

### Initialization: Determining the Initial GMM Parameters

When it comes to initialization of a GMM, we are asking the fundamental question of **what model parameters do we first assign?**. This can be done in different ways, but for GMMs it's very common to first run k-means on your data to get some hard-labels on the data. With these hard-labels, we can use MLE to estimate the component parameters for our initialization (remember MLE works in the "Complete Data" scenario):

* $\mu_{k} = \frac{\sum_{i}^{N_{k}}x_{i,k}}{N_{k}}$
* $\sigma_{k} = \frac{\sum_{i}^{N_{k}}(x_{i,k} - \mu_{k})^2}{N_{k}}$
* $\alpha_{k} = \frac{N_{k}}{N}$

Where $N_{k}$ indicates the number of data points in the kth component. But in GMMs, each data point is not "hard" assigned to any component. They are "soft" assigned and thus belong to each component to a certain degree. In other words, the variable $N_{k}$ doesn't really exist in GMMs. 

Let's try that here:

```{r}
wait.kmeans <- kmeans(wait, 2)
wait.kmeans.cluster <- wait.kmeans$cluster

wait.df <- data_frame(x = wait, cluster = wait.kmeans.cluster)

wait.df %>%
  mutate(num = 1:n()) %>%
  ggplot(aes(y = num, x = x, color = factor(cluster))) +
  geom_point() +
  ylab("Data Point Number") +
  scale_color_discrete(name = "Cluster")
```

Since we specified 2 clusters, k-means nicely splits the data into clusters with means and standard deviation as follows:

```{r}
wait.summary.df <- wait.df %>%
	group_by(cluster) %>%
	summarize(mu = mean(x), std = sd(x), size = n())

wait.summary.df %>%
	dplyr::select(cluster, mu, std)
```

We can also generate the initial alpha values as follows:

```{r}
wait.summary.df <- wait.summary.df %>%
	mutate(alpha = size / sum(size))

wait.summary.df %>%
	dplyr::select(cluster, size, alpha)
```


### Expectation: Calculating the "Soft Labels" of Each Data Point

Now that we have the initial parameters of our GMM, we now have to determine what is the probability (soft label; responsibility) that the data point (x~j~) belongs to component (k~i~)? This is considered the expectation step of MLE where we are calculating the "expectation values" of the soft labels for each data point. 

Mathematically, the question can be posed like this $P(x_{i} \in k_{j} | x_{i})$. How do we actually solve this equation? To help us, we can apply [Bayes' rule](https://en.wikipedia.org/wiki/Bayes%27_rule) here:

$$P(x_{i} \in k_{j} | x_{i}) = \frac{P(x_{i} | x_{i} \in k_{j})P(k_{j})}{P(x_{i})}$$

The parts of this equation are related to the GMM equation above as follows:

* $P(x_{i} | x_{i} \in k_{j})$ = $\mathcal{N}(x_{i}|\mu_{k_{j}},\sigma_{k_{j}})$ (Likelihood)
* $P(k_{j})$ = $\alpha_{k_{j}}$ (Prior)
* $P(x_{i}) = \sum_{k=1}^{K}\alpha_k\mathcal{N}(x_{i}|\mu_{k},\sigma_{k})$ (Marginal Probability)

What we are interested in is $P(x_{i} \in k_{j} | x_{i})$ which is called the posterior probability. Knowing these equations, we can easily calculate this. For instance, what is the posterior of x = 66 belong to the first component? We can first calculate the top part of the equation like this in R:

```{r}
comp1.prod <- 
  dnorm(66, wait.summary.df$mu[1], wait.summary.df$std[1]) *
  wait.summary.df$alpha[1]
```

Here we are using the `dnrom` function from R to make use of the gaussian pdf. To calculate the bottom part of the equation, we actually need to calculate this value for both components and sum them up:

```{r}
comp2.prod <- 
  dnorm(66, wait.summary.df$mu[2], wait.summary.df$std[2]) *
  wait.summary.df$alpha[2]

normalizer <- comp1.prod + comp2.prod
```

Now that we have all the components of the equation, let's plug and solve this:

```{r}
comp1.prod / normalizer
```

We can easily calculate this for every data point as follows:

```{r}
comp1.prod <- dnorm(x = wait, mean = wait.summary.df$mu[1], 
                    sd = wait.summary.df$std[1]) * wait.summary.df$alpha[1]

comp2.prod <- dnorm(x = wait, mean = wait.summary.df$mu[2], 
                    sd = wait.summary.df$std[2]) * wait.summary.df$alpha[2]

normalizer <- comp1.prod + comp2.prod

comp1.post <- comp1.prod / normalizer
comp2.post <- comp2.prod / normalizer
```

### Maximization: Re-estimate the Component Parameters

Now that we have posterior probabilites (i.e. soft labels), we can re-estimate our component parameters. We simply have to make a little adjustment to the MLE equations that we specified early. Specifically, the $N_{k}$ (remember there are no hard labels) is replaced with the posterior probability $P(x_{i} \in k_{j} | x_{i})$ in each equation.

* $\mu_{k} = \frac{\sum_{i}^{N}P(x_{i} | x_{i} \in k_{j})x_{i,k}}{P(x_{i} | x_{i} \in k_{j})}$
* $\sigma_{k} = \frac{\sum_{i}^{N}(P(x_{i} | x_{i} \in k_{j})x_{i,k} - \mu_{k})^2}{P(x_{i} | x_{i} \in k_{j})}$
* $\alpha_{k} = \frac{\sum_{i}^{N}P(x_{i} | x_{i} \in k_{j})}{N}$

With these equations we can now plug in our values and calculate the components parameters using our example from above:

```{r}
comp1.n <- sum(comp1.prod)
comp2.n <- sum(comp2.prod)

comp1.mu <- 1/comp1.n * sum(comp1.post * wait)
comp2.mu <- 1/comp2.n * sum(comp2.post * wait)

comp1.var <- sum(comp1.post * (wait - comp1.mu)^2) * 1/comp1.n
comp2.var <- sum(comp2.post * (wait - comp2.mu)^2) * 1/comp2.n

comp1.alpha <- comp1.n / length(wait)
comp2.alpha <- comp2.n / length(wait)

comp.params.df <- data.frame(comp = c("comp1", "comp2"),
                             comp.mu = c(comp1.mu, comp2.mu),
                             comp.var = c(comp1.var, comp2.var),
                             comp.alpha = c(comp1.alpha, comp2.alpha),
                             comp.cal = c("self", "self"))
```

### Checking for Convergence

As mentioned above, we repeat the expectation and maximization step until we reach "convergence". But what exactly is convergence? The concept of convergence means that we have a change that is minimal enough for us to consider it to neligible and stop running EM. So the question becomes what is the change we are measuring? 

Well since we are fitting trying to fit a GMM to our data, then inituitvely **we should have something that measures the fit of our GMM!** This is actually the final piece of the puzzle. Formally speaking, what we are looking for is called a [cost function](https://en.wikipedia.org/wiki/Loss_function) (aka. objective function, loss function). 

As it turns out, we've already seen the cost function that we need (See "Fitting a GMM using Expectation Maximization"):

$$P(X|\mu,\sigma,\alpha) = \sum_{k=1}^{K}\alpha_k\mathcal{N}(X|\mu_{k},\sigma_{k}^{2})$$

This called the likelihood (see this [post for a good explanation on what a likelihood is](http://alexanderetz.com/2015/04/15/understanding-bayes-a-look-at-the-likelihood/)) and is essentially the fit of your model. Really what we are asking in layman terms is given these model parameters ($\mu,\sigma,\alpha$), what is the probability that our data X was generated by them. A slight modification of this is the log likelihood which equates to:

$$ \ln P(X|\mu,\sigma,\alpha) = \sum_{n=1}^{N}\ln \sum_{k=1}^{K}\alpha_k\mathcal{N}(x_{n}|\mu_{k},\sigma_{k}^{2}) $$

The reason why we do this is because if we simply calculate the likelihood we would end up dealing with very small values which can be problematic. So we take the natural logarithm of the likelihood to circumvent this. What we do is the take the natural log of the sum of the components. Then we sum up all these natural logs. 

> The larger the log likelihood = Better the model parameters fit the data

For instance, the log likelihood of our first EM step:

```{r}
# Already calculate component responsibilities for each data point from above
sum.of.comps <- comp1.prod + comp2.prod
sum.of.comps.ln <- log(sum.of.comps, base = exp(1))
sum(sum.of.comps.ln)
```

So to test for convergency, we can calculate the log likelihood at the end of each EM step (i.e. model fit with these parameters) and then test whether it has changed "significantly" from the last EM step. If it has, then we repeat another step of EM. If not, then we consider that EM has converged and then these are our final parameters.

### Putting it All Together

Now that we have all these pieces of information together, let's put it altogether:

```{r}
#' Expectation Step 
#'
#' Calculate the soft labels (posterior probabilities) that each component
#' has to each data point
e_step <- function(x, mu.vector, sd.vector, alpha.vector) {
  comp1.prod <- dnorm(x, mu.vector[1], sd.vector[1]) * alpha.vector[1]
  comp2.prod <- dnorm(x, mu.vector[2], sd.vector[2]) * alpha.vector[2]
  sum.of.comps <- comp1.prod + comp2.prod
  comp1.post <- comp1.prod / sum.of.comps
  comp2.post <- comp2.prod / sum.of.comps

  sum.of.comps.ln <- log(sum.of.comps, base = exp(1))
  sum.of.comps.ln.sum <- sum(sum.of.comps.ln)

  list("loglik" = sum.of.comps.ln.sum,
       "posterior.df" = cbind(comp1.post, comp2.post))
}

#' Maximization Step
#'
#' Update the Component Parameters
#' @param x Input data
#' @param posterior.df Posterior probability data.frame
m_step <- function(x, posterior.df) {
  comp1.n <- sum(posterior.df[, 1])
  comp2.n <- sum(posterior.df[, 2])

  comp1.mu <- 1/comp1.n * sum(posterior.df[, 1] * x)
  comp2.mu <- 1/comp2.n * sum(posterior.df[, 2] * x)

  comp1.var <- sum(posterior.df[, 1] * (x - comp1.mu)^2) * 1/comp1.n
  comp2.var <- sum(posterior.df[, 2] * (x - comp2.mu)^2) * 1/comp2.n

  comp1.alpha <- comp1.n / length(x)
  comp2.alpha <- comp2.n / length(x)
  list("mu" = c(comp1.mu, comp2.mu),
       "var" = c(comp1.var, comp2.var),
       "alpha" = c(comp1.alpha, comp2.alpha))
}
```

Now we just need to write a loop to go between the functions for each EM step. Each iteration will consist of us first calling the `e_step` function and then calling the `m_step` function if needed. We will run this for 50 iterations or when the log likelihood difference between two iteration is less than `1e-6` (whichever comes first):

```{r}
for (i in 1:50) {
  if (i == 1) {
    # Initialization
    e.step <- e_step(wait, wait.summary.df[["mu"]], wait.summary.df[["std"]], 
                     wait.summary.df[["alpha"]])
    m.step <- m_step(wait, e.step[["posterior.df"]])
    cur.loglik <- e.step[["loglik"]]
    loglik.vector <- e.step[["loglik"]]
  } else {
    # Repeat E and M steps till convergence
    e.step <- e_step(wait, m.step[["mu"]], sqrt(m.step[["var"]]), m.step[["alpha"]])
    m.step <- m_step(wait, e.step[["posterior.df"]])
    loglik.vector <- c(loglik.vector, e.step[["loglik"]])

    loglik.diff <- abs((cur.loglik - e.step[["loglik"]]))
    if(loglik.diff < 1e-6) {
      break
    } else {
      cur.loglik <- e.step[["loglik"]]
    }
  }
}
loglik.vector
```

As you can see, we actual stopped running EM after 16 iterations because the log likelihood didn't change much (specifically, the difference between the 15th and 16th iteration was < 1e6). We classify this as convergence of the algorithm and this represents our final fit. 

## Checking Against mixtools

Let's just make sure ou

## Summary

Mixture models are good at representing data when it comes from a mixture of populations (i.e. heterogenous population). Each cluster is represented mathematically as a parametric distribution (e.g. Guassian distribution). Therefore, the whole model consist of a mixture of distributions; Each distribution is typically referred to as a component distribution. 

In 

The benefit of this approach is that:

* well-studied statistical inference techniques available
* flexibility in choosing the component distribution
* obtain a density estimation for each cluster
* a “soft” classification is available (i.e. probability of each data point belonging to a certain component). This is different from a clustering approach like k-means which assigns a "hard" classification.

In this post, we've demonstrated how to implement your own gaussian mixture model where component is a gaussian. The logic can easily be extended other types of mixture models by simply substituting the components for appropriate distribution needed. 

## Further Reading

This post serves a soft introduction into how to implement your own mixture model. A few things to consider for further reading is:

* Bayesian Mixture Models
* Multi-dimensional Mixture Models - We only considered 1 dimensional mixture models in this post

* 

## References

* [Mixture Models](http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch20.pdf)
* [Expectation Maximization and Gaussian Mixture Models](http://www.slideshare.net/petitegeek/expectation-maximization-and-gaussian-mixture-models)
* [Nature Computational Biology Primer - What is the expectation maximization algorithm?](http://www.nature.com/nbt/journal/v26/n8/full/nbt1406.html)
