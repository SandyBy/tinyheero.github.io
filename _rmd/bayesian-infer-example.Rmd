---
title: "Bayesian Inference Example"
date: "March 17th, 2016"
layout: post
output:
  html_document
tags: [R, stats]
---

```{r message = FALSE, echo = FALSE}
library("knitr")
library("captioner")
library("cowplot")

theme_set(theme_grey())
tbl.nums <- captioner(prefix = "<u>Table</u>")
```

Towards the end of the post [Bayes' Rule]({% post_url 2016-04-21-bayes-rule %}), I eluded a bit to how Bayes' rule becomes extremely poweful in bayesian modeling when we start to intepret the variables of Bayes' rule as parameters of a model and observed data. In this post, we will learn exactly how Bayes' rule is used in bayesian modeling by going through a specific example.

<div class="alert alert-dismissible alert-warning">
<h4>Important</h4>
Please ensure that you are familiar with Bayes' rule before continuing as this post will not make sense with a thorough understanding of it. If you are not familiar, please review my previous post on [Bayes' Rule]({% post_url 2016-04-21-bayes-rule %}) for details.
</div>

Here is an overview of what will be discussed in this post.

**Table of Contents**

<ul data-toc="body" data-toc-headings="h2,h3"></ul>

## The Coin Flipping Example and Steps in Bayesian Analysis

Consider the scenario where you found a coin on the side of a street that had an odd looking geometry, unlike anything you have ever seen before. It still has two sides (heads and a tail), and you start to wonder:

> What the probability of getting a head is on a given flip with this coin?

Given your knowledge of how a typical coin is, your prior guess is that is should be probably 0.5. But given the strange looking geometry, you also entertain the idea that it could be something like 0.4 or 0.6, but think these values are less probable than 0.5. You then proceed to flip the coin 100 times (because you are realy bored and have time on your hands) and you notice that you get 83 heads. Given this observed data now, what now is your guess at the probability of getting a head on a given flip?

If we think back to our examples from the Bayes' Rule post, we can see that this particular example is not that dissimilar:

* We have a prior belief of what the probability of getting a heads is.
* We have some observed data which is 83 heads out of 100 coin tosses.
* We need to update our belief of this probability now that we have some observed data.

There is however one key difference in this particular example compared to our previous examples. **Our prior is not a single fixed value, but rather a series of different possible values. It could be 0.5 or 0.4 or 0.6....in fact it could be any value between 0 and 1!** Moreover, the possible values are not all equally likely. For instance, we have a strong belief that it could be 0.5 (because of what we know about coins in general), and while 0.4 and 0.6 or any other value is possible we still think 0.5 is more probable. Visually, we could have something like:

```{r prior_distr, message = FALSE}
library("dplyr")
library("ggplot2")

#' Generates a "Triangle" Prior Probability Distribution
#'
#' @param vals Sample space of all possible parameter values.
#' @return 2 column data.frame containing the parameter and its corresponding
#'   prior probability.
get_prior_distr <- function(vals) {
  vals.pmin <- pmin(vals, 1 - vals)

  # Normalize the prior so that they sum to 1.
  dplyr::data_frame(theta = vals,
                    prior = vals.pmin / sum(vals.pmin))
}

# Define the Space of all theta values
theta.vals <- seq(0, 1, 0.1)

theta.prior.distr.df <- get_prior_distr(theta.vals)

#' Plots the Prior Probability Distribution
#'
#' @param prior.distr.df Prior probability distribution data.frame from 
#'   get_prior_distr().
#' @param plot.x.labels Plot the parameter values on the x-axes that are taken
#'  from the input data.
plot_prior_distr <- function(prior.distr.df, plot.x.labels = TRUE) {

  theta.prior.p <- 
    prior.distr.df %>%
    ggplot(aes(x = theta, y = prior)) +
    geom_point() +
    geom_segment(aes(x = theta, xend = theta, y = prior, yend = 0)) +
    xlab("theta") +
    ylab("P(theta)") +
    ggtitle("Prior Distribution") 

  if (plot.x.labels) {
    theta.vals <- prior.distr.df[["theta"]]

    theta.prior.p <- 
      theta.prior.p + 
      scale_x_continuous(breaks = c(theta.vals),
                         labels = theta.vals)
  }

  theta.prior.p
}

plot_prior_distr(theta.prior.distr.df)
```

Here we have 10 different possibilities of theta and their associated probabilities. In fact, what we have just described is a probability distribution that was defined in the [Bayes' Rule]({% post_url 2016-03-20-bayes-rule %})! So if we think of our prior as a random variable, <span class="inlinecode">$\theta$</span>, then what we actually have is a prior probability distribution. What implications does this have? Well this ends up affecting how we measure the likelihood because before our likelihood was based on a single prior value. Instead, our likelihood essentially becomes a function of a series of candidate <span class="inlinecode">$\theta$</span> values. In other words, the probability of seeing the observed data is different depending on what the value of <span class="inlinecode">$\theta$</span> is. 

Now that we have given this simple example of a situation, we can walk through an example of how we can answer the aforementioned question (i.e. What the probability of getting a head is on a given flip with this coin?) in the a Bayesian way. To do any Bayesian analysis, we follow a 4 step process:

1. Identify the Observed data you are working with.
1. Construct a probablistic model to represent the data.
1. Specify prior distributions over the parameters of your probablistic model.
1. Collect data and apply Bayes' rule to re-allocate credibility across the possible parameter values.

Let's walk through these steps one by one in the context of this example:

### Step 1: Identify the Observed Data 

In this example, we have a coin that when flipped gives us one of two possible outcomes: heads or tails. We can use the random variable, Y, to denote the outcome and assign it to 1 if it is heads and 0 if it is tails. These values are not ordinal, and merely represent a simplified way to represent heads or tails.

### Step 2: Construct a Probablistic Model to Represent the Data

Once we have identified the observed data, our next step is to come up with a descriptive model along with meaningful parameters that can properly describe this data. The idea here is that the model is supposed to represent some generative process that generated this data. For this particular example, we can represent the probability of getting a head as <span class="inlinecode">$\theta$</span>. Formally we can describe this as:

<div>
$$P(Y = 1 |\ \theta) = \theta$$
</div>

It then follows that the probability of getting a tail is:

<div>
$$P(Y = 0 |\ \theta) = 1 - \theta$$
</div>

These two expressions can be combined into a single expression:

<div>
$$P(y\ |\ \theta) = \theta^{y}(1 - \theta)^{1 - y}$$
</div>

Notice how when <span class="inlinecode">$Y = 1$</span>, this expression becomes <span class="inlinecode">$P(Y = 1 |\ \theta) = \theta$</span>. And when <span class="inlinecode">$Y = 0$</span>, this expression becomes <span class="inlinecode">$P(Y = 0 |\ \theta) = 1 - \theta$</span>.

This particular expression is actually the probability mass function of the Bernouli distribution. The Bernouli distribution is used when we are describing a single trial (e.g. coin flip) with two possible outcomes (e.g. heads or tails). We can extend this to the situation where we have multiple independent trials very easily. First, we let <span class="inlinecode">$y_{i}$</span> represent the outcome of the ith coin flip and the set of all outcomes to be <span class="inlinecode">${y_{i}}$</span>. Since each coin toss is an independent (i.e. the outcome of a coin toss is independent of the previous coin toss outcomes), the probability of <span class="inlinecode">${y_{i}}$</span> is then multiplicative product of the individual outcomes:

<div>
$$\begin{align}
P({y_{i}}\ |\ \theta) &= \prod{p(y_{i}\ |\ \theta)
P(Y\ |\ X)\ P(X) &= P(X, Y)
\end{align}$$
</div>

This expression is the probability mass function (pmf) for the binomial distribution. As we are dealing with multiple coin flips here, the binomial distribution serves as the perfect model for this data. As such, we can use the pmf of the binomial distribution to be our likelihood function for Bayes' rule.

<div class="alert alert-dismissible alert-warning">
<h4>Cool Fact</h4>
The Bernouli distribution is just a special case of the binomial distribution when there is only one trial.
</div>

### Step 3: Specify Prior Distributions 

Now that we have specified a probabilistic model to represent the coin toss, we have to consider the parameters of this model. The model is comprised of a single binomial distribution PMF which is parameterized by a single <span class="inlinecode">$\theta$</span>. So we need to define some possible values that <span class="inlinecode">$\theta$</span> can take.

For this example, we will restrict our parameter values to discrete values of <span class="inlinecode">$\theta = 0, \theta = 0.1, ..., \theta = 1.0$</span>. And we believe that the certain parameters are more likely. For instance, the probability of the coin being "fair" <span class="inlinecode">$\theta = 0.5$</span> is more likely than a coin being unfair. So we could define a probability distribution as follows:

```{r prior_distr_triangle, message = FALSE}
plot_prior_distr(theta.prior.distr.df)
```

This is called a "prior probability distribution" and defines the possibilities of <span class="inlinecode">$\theta$</span> and their associated probabilities.

### Step 4: Collect Data and Application of Bayes' Rule

The final step is that we use the observed data and apply Bayes' rule to generate the posterior distribution. Let's start with the simplest scenario which is a single coin toss which has the outcome of a head. We now use Bayes' rule to generate a posterior for each <span class="inlinecode">$\theta$</span> value:

```{r posterior_prob_distr/setup, message = FALSE}
#' Get the Likelihood Probability Distribution
#'
#' Generates a likelihood probability distribution data frame
#'
#' @param theta.vals Vector of theta values for the binomial distribution.
#' @param num.heads Number of heads.
#' @param num.tails Number of tails.
#' @return data_frame for the likelihood probability distribution.
get_likelihood_df <- function(theta.vals, num.heads, num.tails) {
  likelihood.vals <- c()
  for (cur.theta.val in theta.vals) {
    likelihood.vals <- 
      c(likelihood.vals, 
        (cur.theta.val^num.heads) * (1 - cur.theta.val)^(num.tails))
  }

  likelihood.df <- dplyr::data_frame(theta = theta.vals,
                                     likelihood = likelihood.vals)
  likelihood.df
}

#' Get Posterior Probability Distribution
#' 
#" Generate a posterior probability distribution data.frame.
#'
#' @param likelihood.df Likelihood distribution data.frame from 
#'   get_likelihood_df().
#' @param theta.prior.distr.df Prior distribution data.frame from 
#'   get_prior_distr().
#' @return data_frame of the posterior probability distribution.
get_posterior_df <- function(likelihood.df, prior.distr.df) {

  marg.likelihood <- sum(likelihood.df[["likelihood"]])

  posterior.df <- 
    likelihood.df %>%
    dplyr::left_join(prior.distr.df) %>%
    dplyr::mutate(marg_likelihood = marg.likelihood) %>%
    dplyr::mutate(post_prob = (likelihood * prior) / marg.likelihood)

  posterior.df
}

#' Plots Likelihood Probability Distribution
plot_likelihood_prob_distr <- 
  data.frame() %>%
  ggplot(aes(x = theta, y = likelihood)) +
  geom_point() +
  geom_segment(aes(x = theta, xend = theta, y = likelihood, yend = 0)) +
  xlab("theta") +
  ylab("P(theta)") +
  ggtitle("Likelihood Distribution")

#' Plots Posterior Probability Distribution
plot_posterior_prob_distr <- function(posterior.df, theta.vals) {
  posterior.df %>%
  ggplot(aes(x = theta, y = post_prob)) +
  geom_point() +
  geom_segment(aes(x = theta, xend = theta, y = post_prob, yend = 0)) +
  xlab("theta") +
  ylab("P(theta | Data)") +
  #scale_x_continuous(breaks = c(theta.vals),
  #                   labels = theta.vals) +
  ggtitle("Posterior Distribution")
}

likelihood.df <- get_likelihood_df(theta.vals, 1, 0)
posterior.df <- get_posterior_df(likelihood.df, theta.prior.distr.df)
```

```{r posterior_prob_distr, message = FALSE}
plot_posterior_prob_distr(posterior.df, theta.vals)
```

Notice how the posterior probability distribution is different from the prior probability distribution. Specifically, notice how the probability mass has shifted to higher <span class="inlinecode">$\theta$</span> values. This makes sense since that the outcome was head suggesting that the coin favours heads. Hence, <span class="inlinecode">$\theta$</span> values supporting a bias towards head outcomes will be favoured and thus we see the mass of the posterior distribution shifting towards that direction.

#### Influence of Sample Size on Bayesian Inference

Importantly, our observed data is only 1 coin toss which gave us a single outcome which was head. This isn't a lot of information to work with in trying to determine the posterior distribution. Let's see what happens when we have 20 coin toss and with a total of 15 heads.

```{r posterior_prob_distr/sample_size, message = FALSE}
likelihood.df <- get_likelihood_df(theta.vals, 15, 5)

plot_likelihood_prob_distr %+%
  likelihood.df
```

Notice in this situation how the mass has shifted even more to the right side and in particular the majority of it is on 0.7 and 0.8. This makes sense again because the data suggest that <span class="inlinecode">$\theta$</span> should be around 0.75. The reason why we do not see mass at 0.75 is because we restricted our parameter space to discrete values of <span class="inlinecode">$\theta = 0, \theta = 0.1, ..., \theta = 1.0$</span>. We can easily expand our parameter space a larger "grid":

```{r message = FALSE}
new.theta.vals <- seq(0, 1, 0.001)
new.prior.distr.df <- get_prior_distr(new.theta.vals)
new.likelihood.df <- get_likelihood_df(new.theta.vals, 15, 5)
new.posterior.df <- get_posterior_df(new.likelihood.df, new.prior.distr.df)

#new.prior.distr.df

plot_prior_distr(new.prior.distr.df, plot.x.labels = FALSE)

plot_posterior_prob_distr(new.posterior.df, new.theta.vals)
```

## Putting It All Together

```{r}
#plot_grid(theta.prior.p, likelihood.p, posterior.p, nrow = 3)
```

## What Makes Bayesian Inference Difficult

In this work

## Bayesian vs. Frequentist Thinking

where frequentist sees the parameter(s), <span class="inlinecode">$\theta$</span>, as being fixed and the observed data as being variable.
This segues into an important concept in Bayesian data analysis which is **the observed data, Y, is see as being fixed and the parameter(s), <span class="inlinecode">$\theta$</span>, as the variable.** This is the fundamental difference between bayesian thinking and frequentist thinking. We will touch about this later on in the post.



## Conclusion

We've used coin flipping as our example for Bayesian modeling here, but it's important to understand that even if we don't care about coin flipping 

## References

 [Count Bayesie - Bayes' Theorem with Lego](https://www.countbayesie.com/blog/2015/2/18/bayes-theorem-with-lego)

## R Session

```{r session, echo = FALSE}
devtools::session_info()
```
