---
title: "Implementating a Gaussian Mixture Model in R"
author: "Fong Chun Chan"
date: "August 25, 2015"
output: html_document
---

## TL;DR - Summary

* Learn why you would use mixture model clustering
* Use gaussian mixture model clustering from the mixtools R package
* Implement your own gaussian mixture model in R

```{r load_packages}
library("ggplot2")
library("dplyr")
library("mixtools")

plot_mix_comps <- function(x, mu, sigma, lam) {
  lam * dnorm(x, mu, sigma)
}
```

## Why would you need Mixture Model Clustering?

Let's say someone presented you with the following density plot: 

```{r, message = FALSE}
options(scipen = 999)
p <- ggplot(faithful, aes(x = waiting)) +
	geom_density()
p
```

We can immediately see that the resulting distribution appears to be bi-modal (i.e. there are two bumps) suggesting that these data might be coming from two different sources. These data are actually from the `faithful` dataset available in R:

```{r}
head(faithful)
```

This data is 2-column data.frame 

* eruptions: Length of eruption (in mins)
* waiting: Time in between eruptions (in mins)

Putting the data into context suggests that the eruption times may be coming from two different sub-populations. There could be several reasons for this. For instance, maybe at different times of the year the geyser eruptions are more frequent. You can probably take an intutive guess as to how you could split this data. 

For instance, there likely is a population with a mean eruption of ~53 with some variance around this mean (red vertical line in figure below.) Another population with a mean eruption of ~80 with again some variance around this mean (black vertical line in figure below).

```{r}
p + 
	geom_vline(x = 53, col = "red") + 
	geom_vline(x = 80)
```

In fact, what we've done is a naive attempt at trying to group the data into different populations/clusters. In the next section, we will actually perform a machine learning technique to do the clustering.

## Using Gaussian Mixture Model Clustering

But rather than take a subjective way to determine this, we can employ a machine-learning technique called [**mixture model clustering**](https://en.wikipedia.org/wiki/Mixture_model).  This is a "model-based approach" to clustering which uses statistical distributions to cluster data and attempt to optimize the fit between the data and the model. One of the most common statistical distributions used for mixture model clustering is the [Gaussian/Normal Distribution](https://en.wikipedia.org/wiki/Normal_distribution):

$$ \mathcal{N}(\mu, \sigma^2) $$

The normal distribution is parameterized by two variables:

* $\mu$: Mean; Center of the mass
* $\sigma^2$: Variance; Spread of the mass

When Gaussians are used for mixture model clustering they are referred to as [Gaussian Mixture Models (GMM)](https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model). As it turns out, our early intuition on where the means and variance of the subpopulation in the plot above is a perfect example of how we could apply a GMM. Specifically, we could try to represent each subpopulation as its own distribution (aka. mixture component). The entire set of data could then be represented as a mixture of 2 Gaussian distributions (aka. 2-component GMM)

In R, there are several packages that provide an implementation of GMM already (e.g. [mixtools](https://cran.r-project.org/web/packages/mixtools/index.html), [mclust](http://www.stat.washington.edu/mclust/)). As there exists [a nice blog post by Ron Pearson](http://exploringdatablog.blogspot.ca/2011/08/fitting-mixture-distributions-with-r.html) on using mixttools on the `faithful` dataset, we will just borrow a bit his code to demonstrate the GMM in action:

```{r mixtools, message = FALSE}
set.seed(1)
wait <- faithful$waiting
mixmdl <- normalmixEM(wait, k = 2)
#plot(mixmdl, which = 2)

data.frame(x = mixmdl$x) %>%
  ggplot() +
  geom_histogram(aes(x, ..density..), binwidth = 1, colour = "black", 
                 fill = "white") +
  stat_function(geom = "line", fun = plot_mix_comps,
                arg = list(mixmdl$mu[1], mixmdl$sigma[1], lam = mixmdl$lambda[1]),
                colour = "red", lwd = 1.5) +
  stat_function(geom = "line", fun = plot_mix_comps,
                arg = list(mixmdl$mu[2], mixmdl$sigma[2], lam = mixmdl$lambda[2]),
                colour = "blue", lwd = 1.5) +
  ylab("Density")
```

The key is the `normalmixEM` function which builds a 2-component GMM (`k = 2` indicates to use 2 components). So how do we interpret this? It's actually quite simply; The red and blue lines simply indicate 2 different fitted Gaussian distributions. Specifically, the means of the 2 Gaussians (red and green) are respectively:

```{r}
mixmdl$mu
```

With respectively variances of:

```{r}
mixmdl$sigma
```

You might also notice how the "heights" of the two components (herein we will refer to distribution as component) are different. Specifically, the green component is "higher" than the red component. This is because the green component encapsulates more density (i.e. more data) compared to the red component. How much exactly? You can get this value by using:

```{r}
mixmdl$lambda
```

Formally, these are referred to as the mixing weights (aka. mixing proportions, mixing coefficients). One can interpret this as the red component representing `r round(mixmdl$lambda[1]*100, 3)`% and the green component representing `r round(mixmdl$lambda[2]*100, 3)`% of the input data. Another important aspect is that each input data point is actually assigned a posterior probability of belonging to one of these components. We can retrieve these data by using the following code:

```{r}
post.df <- as.data.frame(cbind(x = mixmdl$x, mixmdl$posterior))
head(post.df, 10)  # Retrieve first 10 rows
```

The x column indicates the value of the data while comp.1 and comp.2 refers to the posterior probability of belonging to either component respectively. If you look at the x value in the first row, `r post.df[1, "x"]`, you will see that it sits pretty close to the middle of the green component (the mean of the green component `r round(mixmdl$mu[2], 3)`). So it makes sense that the posterior of this data point belonging to this component should be high (`r round(post.df[1, "comp.2"], 5)` vs. `r round(post.df[1, "comp.1"], 5)`). And simiarly, the data that sits inbetween the two components will have posterior probabilities that are not strongly associated with either component:

```{r, message = FALSE}
library("dplyr")
post.df %>%
	filter(x > 66, x < 68)
```

It's important to understand that no "labels" have been assigned here actually. Unlike k-means which assigns each data point to a cluster (defined as a "hard-label"), Mixture models provide what are called "soft-labels". The end-user decides on what "threshold" to use to assign data into the components. For instance, one could use 0.3 as posterior threshold to assign data to comp.1 and get the following label distribution.

```{r}
post.df %>%
	mutate(label = ifelse(comp.1 > 0.3, 1, 2)) %>% 
	ggplot(aes(x = factor(label))) +
	geom_bar() +
	xlab("Component") +
	ylab("Number of Data Points")
```

Or one could use 0.8 and get the following label distribution:

```{r}
post.df %>%
	mutate(label = ifelse(comp.1 > 0.8, 1, 2)) %>%
	ggplot(aes(x = factor(label))) +
	geom_bar() +
	xlab("Component") +
	ylab("Number of Data Points")
```

## Implementing Your Own GMM In R 

If you are like me, then not knowing what is happening "under the hood" may bug you. What is actually happening when I run `normalmixEM`? Where are these posterior probabilites coming from? How does it fit the components? To understand all this, we need to first understand the mathematical representation of a GMM:

$$P(X|\mu,\sigma,\alpha) = \sum_{k=1}^{K}\alpha_k\mathcal{N}(X|\mu_{k},\sigma_{k}^{2})$$ 

* X = Dataset of n elements (x~1~, ..., x~n~).
* K = Number of components.
* $\alpha_{k}$ = Mixing weight of the kth component. $\sum_{k=1}^{K}\alpha_{k} = 1$.
* $\mathcal{N}(x|\mu_{k},\sigma_{k})$ Gaussian probability density function (pdf) of the kth component defined by the parameters $\mu_{k}$ and $\sigma_{k}$.
* $\mu_{k}$ = Mean of the kth component.
* $\sigma_{k}^{2}$ = Variance of the kth component.

It looks complicated. but if we use our example from above then this breaksdown to:

$$P(X) = `r round(mixmdl$lambda[1], 3)`\mathcal{N}(X|`r round(mixmdl$mu[1], 3)`, `r round(mixmdl$sigma[1], 3)`) + `r round(mixmdl$lambda[2], 3)`\mathcal{N}(X|`r round(mixmdl$mu[2], 3)`, `r round(mixmdl$sigma[2], 3)`)$$

All we've done here is substitute in the parameters of the mixture model. 

In case you were wondering what the $P(X|\mu,\sigma,\alpha)$ means, don't worry about that for now. We will explain exactly what this means later on in the post.

### Calculating the Posterior Probabilities

If you were given all the parameters of the mixture model (e.g. $\alpha_{1}$, ..., $\alpha_{k}$, $\mu_{1}$, ..., $\mu_{k}$, $\sigma_{1}$, ...$\sigma_{k}$), you would be able to ask the simple question:

> What is the probability that the data point (x~j~) belongs to component (k~i~)?

Mathematically, the question can be posed like this $P(x_{i} \in k_{j} | x_{i})$. How do we actually solve this equation? To help us, we can apply [Bayes' rule](https://en.wikipedia.org/wiki/Bayes%27_rule) here:

$$ P(x_{i} \in k_{j} | x_{i}) = \frac{P(x | x \in k_{j})P(k_{j})}{P(x_{i})} $$

This parts of this equation are related to the GMM equation above as follows:

* $P(x_{i} | x_{i} \in k_{j})$ = $\mathcal{N}(x_{i}|\mu_{k_{j}},\sigma_{k_{j}})$. This is known as the "responsibility": How much is the kth component responsible for the x~i~ data point.
* $P(k_{j})$ = $\alpha_{k_{j}}$
* $P(x_{i}) = \sum_{k=1}^{K}\alpha_k\mathcal{N}(x_{i}|\mu_{k},\sigma_{k})$

Knowing these equations now, let's see if we can calculate the posterior values from the example above. For instance, what is the posterior of x = `r mixmdl$x[33]` belong to the green component? We can first calculate the top part of the equation like this in R:

```{r}
comp1.prod <- dnorm(mixmdl$x[33], mixmdl$mu[1], mixmdl$sigma[1]) * 
  mixmdl$lambda[1]
```

Here we are using the `dnrom` function from R to make use of the gaussian pdf. To calculate the bottom part of the equation, we actually need to calculate this value for both components and sum them up:

```{r}
comp2.prod <- dnorm(mixmdl$x[33], mixmdl$mu[2], mixmdl$sigma[2]) * 
  mixmdl$lambda[2]

normalizer <- comp1.prod + comp2.prod
```

Now that we have all the components of the equation, let's plug and solve this:

```{r}
comp1.prod / normalizer
```

We can easily calculate this for every data point as follows:

```{r}
comp1.prod <- dnorm(x = mixmdl$x, mean = mixmdl$mu[1], 
										sd = mixmdl$sigma[1]) * mixmdl$lambda[1]

comp2.prod <- dnorm(x = mixmdl$x, mean = mixmdl$mu[2], 
										sd = mixmdl$sigma[2]) * mixmdl$lambda[2]

normalizer <- comp1.prod + comp2.prod

comp1.post <- comp1.prod / normalizer
comp2.post <- comp2.prod / normalizer
```

Let's see how our own calculations compare to the mixtools reported posteriors to make sure that we are doing it right:

```{r}
post.df %>%
  mutate(comp.1.recal = comp1.post) %>%
  ggplot(aes(x = comp.1, y = comp.1.recal)) +
  geom_point() +
  xlab("Mixtools Component 1 Posterior") +
  ylab("Recalulated Compponent 1 Posterior") +
  ggtitle("Recalulated vs. Mixtools Component 1 Posteriors")
```

```{r}
post.df %>%
  mutate(comp.2.recal = comp2.post) %>%
  ggplot(aes(x = comp.2, y = comp.2.recal)) +
  geom_point() +
  xlab("Mixtools Component 2 Posterior") +
  ylab("Recalulated Compponent 2 Posterior") +
  ggtitle("Recalulated vs. Mixtools Component 2 Posteriors")
```

### Calculating the Component Parameters

In the previous section, we learned how to calculate the component posterior probabilites for each data point. But before we can do this, we need to actually have parameters of the components. How do we "fit" the components to the data to get the parameters? 

Let's start with a simplier question. If we knew which data points belonged to which components, what would the parameters of the components be? We can just use maximum likelihood estimation to determine the components parameters, so:

* $\mu_{k} = \frac{\sum_{i}^{N_{k}}x_{i,k}}{N_{k}}$
* $\sigma_{k} = \frac{\sum_{i}^{N_{k}}(x_{i,k} - \mu_{k})^2}{N_{k}}$

Where $N_{k}$ indicates the number of data points in the kth component. But in GMMs, each data point is not "hard" assigned to any component. They are "soft" assigned and thus belong to each component to a certain degree. In other words, t he variable $N_{k}$ doesn't really exist in GMMs. 

So how do we do this? If you remember the posterior probability calculations from the previous section, there essentially server as the soft assignments. We can make use of these data by incorporating it into the calculations as follows:

* $\mu_{k} = \frac{\sum_{i}^{N}P(x_{i} | x_{i} \in k_{j})x_{i,k}}{P(x_{i} | x_{i} \in k_{j})}$
* $\sigma_{k} = \frac{\sum_{i}^{N}(P(x_{i} | x_{i} \in k_{j})x_{i,k} - \mu_{k})^2}{P(x_{i} | x_{i} \in k_{j})}$

Essentially, we are incorporting an "uncertainity" in the association of a data point with a component. If we were 100% certain of the labels of each cluster, then we would just multiple each data point by 1 we would end by the same "hard" label equations as above.

Now that we know this, let's calculate the components parameters using our example from above and compare them to the mixtools parameters:

```{r}
comp1.n <- sum(mixmdl$posterior[, 1])
comp2.n <- sum(mixmdl$posterior[, 2])

comp1.mu <- 1/comp1.n * sum(mixmdl$posterior[, 1] * mixmdl$x)
comp2.mu <- 1/comp2.n * sum(mixmdl$posterior[, 2] * mixmdl$x)

comp1.var <- sum(mixmdl$posterior[, 1] * (mixmdl$x - comp1.mu)^2) * 1/comp1.n
comp2.var <- sum(mixmdl$posterior[, 2] * (mixmdl$x - comp2.mu)^2) * 1/comp2.n

comp.params.df <- data.frame(comp = c("comp1", "comp2"),
                             comp.mu = c(comp1.mu, comp2.mu),
                             comp.var = c(comp1.var, comp2.var),
                             comp.cal = c("self", "self"))

mixtools.comp.params.df <- data.frame(comp = c("comp1", "comp2"),
                                      comp.mu = mixmdl$mu, 
                                      comp.var = mixmdl$sigma^2,
                                      comp.cal = c("mixtools", "mixtools"))
```

```{r}
library("reshape2")
library("dplyr")

rbind(comp.params.df, mixtools.comp.params.df) %>%
  melt %>%
  ggplot(aes(x = comp.cal, y = value)) +
  facet_grid(comp ~ variable) +
  geom_bar(stat = "identity")
```

How do these parameters compare to the parameters reported by mixtools?

```{r}
mixmdl$mu
mixmdl$sigma^2
```

> Note that mixtools reports the standard deviation of the components. So we squared the values to get the variance.

### Determining the Best Fit

In the last two sections, we learned how to calculate the posteriors probabilities of each data point and the parameters of the components. But we still have one outstanding question which is how do we know that these posteriors and components parameters are the "best fit". For instance, let's say we just shifted the components by 3 (this is just for demonstrative purposes and mathematically doesn't reall represent a proper solution):

```{r}
data.frame(x = mixmdl$x) %>%
  ggplot() +
  geom_histogram(aes(x, ..density..), binwidth = 1, colour = "black", 
                 fill = "white") +
  stat_function(geom = "line", fun = plot_mix_comps,
                arg = list(mixmdl$mu[1] - 3, mixmdl$sigma[1], lam = mixmdl$lambda[1]),
                colour = "red", lwd = 1.5) +
  stat_function(geom = "line", fun = plot_mix_comps,
                arg = list(mixmdl$mu[2] - 3, mixmdl$sigma[2], lam = mixmdl$lambda[2]),
                colour = "blue", lwd = 1.5) +
  ylab("Density")
```

How do know this isn't a better fit than before? I think initutive you can agree that the results that came from mixtools look better than this. But how does mixtools know where to "put" the components? What we are asking here is actual a question of [model selection](https://en.wikipedia.org/wiki/Model_selection): Is model A better than model B?

To compare model performance, we first need to determine how to evaluate the fit of a particular model. How we do this depends entirely on the model you are working with. In our case, the equation we need was actually already presented above. 

$$P(X|\mu,\sigma,\alpha) = \sum_{k=1}^{K}\alpha_k\mathcal{N}(X|\mu_{k},\sigma_{k}^{2})$$ 

This called the likelihood. Really what we are asking in layman terms is given these model parameters ($\mu,\sigma,\alpha$), what is the probability that our data X was generated by them:

```{r}
mixmdl$all.loglik
mixmdl
comp1.prod + comp2.prod

comp1.prod <- dnorm(x = mixmdl$x, mean = mixmdl$mu[1], 
										sd = mixmdl$sigma[1]) * mixmdl$lambda[1]

comp2.prod <- dnorm(x = mixmdl$x, mean = mixmdl$mu[2], 
										sd = mixmdl$sigma[2]) * mixmdl$lambda[2]
comp2.prod

sum(log(comp1.prod + comp2.prod, base = exp(1)))

mixmdl$loglik

sum(log(comp1.prod + comp1.prod))

comp1.post

```



$$P(\mu,\sigma,\alpha | X)$$

This is k

## Summary

It is good at representing data when it comes from a mixture of populations (i.e. heterogenous population). Each cluster is represented mathematically as a parametric distribution (e.g. Guassian distribution). Therefore, the whole model consist of a mixture of distributions; Each distribution is typically referred to as a component distribution. 

The benefit of this approach is that:

* well-studied statistical inference techniques available
* flexibility in choosing the component distribution
* obtain a density estimation for each cluster
* a “soft” classification is available (i.e. probability of each data point belonging to a certain component). This is different from a clustering approach like k-means which assigns a "hard" classification.

## References

* [Expectation Maximization and Gaussian Mixture Models](http://www.slideshare.net/petitegeek/expectation-maximization-and-gaussian-mixture-models)
