---
title: "Implementating a Gaussian Mixture Model in R"
author: "Fong Chun Chan"
date: "August 25, 2015"
output: html_document
---

## TL;DR - Summary

* Learn why you would use mixture model clustering
* Use gaussian mixture model clustering from the mixtools R package
* Implement your own gaussian mixture model in R

## Why would you need Mixture Model Clustering?

Let's say someone presented you with the following density plot: 

```{r, message = FALSE}
options(scipen = 999)
library("ggplot2")
p <- ggplot(faithful, aes(x = waiting)) +
	geom_density()
p
```

We can immediately see that the resulting distribution appears to be bi-modal (i.e. there are two bumps) suggesting that these data might be coming from two different sources. These data are actually from the `faithful` dataset available in R:

```{r}
head(faithful)
```

This data is 2-column data.frame 

* eruptions: Length of eruption (in mins)
* waiting: Time in between eruptions (in mins)

Putting the data into context suggests that the eruption times may be coming from two different sub-populations. There could be several reasons for this. For instance, maybe at different times of the year the geyser eruptions are more frequent. You can probably take an intutive guess as to how you could split this data. 

For instance, there likely is a population with a mean eruption of ~53 with some variance around this mean (red vertical line in figure below.) Another population with a mean eruption of ~80 with again some variance around this mean (black vertical line in figure below).

```{r}
p + 
	geom_vline(x = 53, col = "red") + 
	geom_vline(x = 80)
```

In fact, what we've done is a naive attempt at trying to group the data into different populations/clusters. In the next section, we will actually perform a machine learning technique to do the clustering.

## Using Gaussian Mixture Model Clustering

But rather than take a subjective way to determine this, we can employ a machine-learning technique called [**mixture model clustering**](https://en.wikipedia.org/wiki/Mixture_model).  This is a "model-based approach" to clustering which uses statistical distributions to cluster data and attempt to optimize the fit between the data and the model. One of the most common statistical distributions used for mixture model clustering is the [Gaussian/Normal Distribution](https://en.wikipedia.org/wiki/Normal_distribution):

$$ \mathcal{N}(\mu, \sigma^2) $$

The normal distribution is parameterized by two variables:

* $\mu$: Mean; Center of the mass
* $\sigma^2$: Variance; Spread of the mass

When Gaussians are used for mixture model clustering they are referred to as [Gaussian Mixture Models (GMM)](https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model). As it turns out, our early intuition on where the means and variance of the subpopulation in the plot above is a perfect example of how we could apply a GMM. Specifically, we could try to represent each subpopulation as its own distribution (aka. mixture component). The entire set of data could then be represented as a mixture of 2 Gaussian distributions (aka. 2-component GMM)

In R, there are several packages that provide an implementation of GMM already (e.g. [mixtools](https://cran.r-project.org/web/packages/mixtools/index.html), [mclust](http://www.stat.washington.edu/mclust/)). As there exists [a nice blog post by Ron Pearson](http://exploringdatablog.blogspot.ca/2011/08/fitting-mixture-distributions-with-r.html) on using mixttools on the `faithful` dataset, we will just borrow a bit his code to demonstrate the GMM in action:

```{r mixtools, message = FALSE}
library("mixtools")
set.seed(1)
wait <- faithful$waiting
mixmdl <- normalmixEM(wait, k = 2)
plot(mixmdl, which = 2)
```

The key is the `normalmixEM` function which builds a 2-component GMM (`k = 2` indicates to use 2 components). So how do we interpret this? It's actually quite simply; The red and green lines simply indicate 2 different fitted Gaussian distributions. Specifically, the means of the 2 Gaussians (red and green) are respectively:

```{r}
mixmdl$mu
```

With respectively variances of:

```{r}
mixmdl$sigma
```

You might also notice how the "heights" of the two components (herein we will refer to distribution as component) are different. Specifically, the green component is "higher" than the red component. This is because the green component encapsulates more density (i.e. more data) compared to the red component. How much exactly? You can get this value by using:

```{r}
mixmdl$lambda
```

Formally, these are referred to as the mixing weights (aka. mixing proportions, mixing coefficients). One can interpret this as the red component representing `r round(mixmdl$lambda[1]*100, 3)`% and the green component representing `r round(mixmdl$lambda[2]*100, 3)`% of the input data. Another important aspect is that each input data point is actually assigned a posterior probability of belonging to one of these components. We can retrieve these data by using the following code:

```{r}
post.df <- as.data.frame(cbind(x = mixmdl$x, mixmdl$posterior))
head(post.df, 10)  # Retrieve first 10 rows
```

The x column indicates the value of the data while comp.1 and comp.2 refers to the posterior probability of belonging to either component respectively. If you look at the x value in the first row, `r post.df[1, "x"]`, you will see that it sits pretty close to the middle of the green component (the mean of the green component `r round(mixmdl$mu[2], 3)`). So it makes sense that the posterior of this data point belonging to this component should be high (`r round(post.df[1, "comp.2"], 5)` vs. `r round(post.df[1, "comp.1"], 5)`). And simiarly, the data that sits inbetween the two components will have posterior probabilities that are not strongly associated with either component:

```{r, message = FALSE}
library("dplyr")
post.df %>%
	filter(x > 66, x < 68)
```

It's important to understand that no "labels" have been assigned here actually. Unlike k-means which assigns each data point to a cluster (defined as a "hard-label"), Mixture models provide what are called "soft-labels". The end-user decides on what "threshold" to use to assign data into the components. For instance, one could use 0.3 as posterior threshold to assign data to comp.1 and get the following label distribution.

```{r}
post.df %>%
	mutate(label = ifelse(comp.1 > 0.3, 1, 2)) %>% 
	ggplot(aes(x = factor(label))) +
	geom_bar() +
	xlab("Component") +
	ylab("Number of Data Points")
```

Or one could use 0.8 and get the following label distribution:

```{r}
post.df %>%
	mutate(label = ifelse(comp.1 > 0.8, 1, 2)) %>%
	ggplot(aes(x = factor(label))) +
	geom_bar() +
	xlab("Component") +
	ylab("Number of Data Points")
```

## Implementing Your Own GMM In R 

If you are like me, then not knowing what is happening "under the hood" may bug you. What is actually happening when I run `normalmixEM`? Where are these posterior probabilites coming from? How does it fit the components? To understand all this, we need to first understand the mathematical representation of a GMM:

$$P(X) = \sum_{k=1}^{K}\alpha_k\mathcal{N}(X|\mu_{k},\sigma_{k})$$ 

* X = Dataset of n elements (x~1~, ..., x~n~).
* K = Number of components.
* $\alpha_{k}$ = Mixing weight of the kth component. $\sum_{k=1}^{K}\alpha_{k} = 1$.
* $\mathcal{N}(x|\mu_{k},\sigma_{k})$ Gaussian probability density function (pdf) of the kth component defined by the parameters $\mu_{k}$ and $\sigma_{k}$.
* $\mu_{k}$ = Mean of the kth component.
* $\sigma_{k}$ = Standard deviation of the kth component.

It looks complicated. but if we use our example from above then this breaksdown to:

$$P(X) = `r round(mixmdl$lambda[1], 3)`\mathcal{N}(X|`r round(mixmdl$mu[1], 3)`, `r round(mixmdl$sigma[1], 3)`) + `r round(mixmdl$lambda[2], 3)`\mathcal{N}(X|`r round(mixmdl$mu[2], 3)`, `r round(mixmdl$sigma[2], 3)`)$$

All we've done here is substitute in the parameters of the mixture model. If you were given all the parameters of the mixture model (e.g. $\alpha_{1}$, ..., $\alpha_{k}$, $\mu_{1}$, ..., $\mu_{k}$, $\sigma_{1}$, ...$\sigma_{k}$), you would be able to ask the simple question:

> What is the probability that the data point (x~j~) belongs to component (k~i~)?

Mathematically, the question be posed like this $P(x_{j} \in k_{i} | x_{j})$. We can [Bayes rule](https://en.wikipedia.org/wiki/Bayes%27_rule) to help us here. Once we apply Bayes rule, we get the following equation:

$$ P(x_{j} \in k_{i} | x_{j}) = \frac{P(x | x \in k_{i})P(k_{i})}{P(x)} $$



you would be able to literally plug and play and get a final "value" of this GMM equation.

### Calculating the Posterior Probabilities of Each Data Point



As we've already seen, we can get the posterior probabilites of each data point by using `mixmdl$posterior`. But how is this actually generated? The mathematical equation to calculate the posterior probability of x data point belong to component i is as follows:






### Calcul


## Summary

It is good at representing data when it comes from a mixture of populations (i.e. heterogenous population). Each cluster is represented mathematically as a parametric distribution (e.g. Guassian distribution). Therefore, the whole model consist of a mixture of distributions; Each distribution is typically referred to as a component distribution. 

The benefit of this approach is that:

* well-studied statistical inference techniques available
* flexibility in choosing the component distribution
* obtain a density estimation for each cluster
* a “soft” classification is available (i.e. probability of each data point belonging to a certain component). This is different from a clustering approach like k-means which assigns a "hard" classification.

## References
